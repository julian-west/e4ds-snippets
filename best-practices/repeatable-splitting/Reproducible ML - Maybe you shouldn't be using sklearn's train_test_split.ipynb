{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16461af8",
   "metadata": {},
   "source": [
    "# ML Design Pattern: Repeatable Splitting\n",
    "\n",
    "\n",
    "## Reproducible ML: Maybe you shouldn't be using train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa138163",
   "metadata": {},
   "source": [
    "Reproducibility is critical for robust data science -- after all, it is a science.\n",
    "\n",
    "But reproducibility in ML can be surprisingly difficult. \n",
    "\n",
    "**The behaviour of your model doesn't only depend on your code, but also the underlying dataset that was used to train it**\n",
    "\n",
    "Therefore, you need to keep tight control on which datapoints were used to train and test your model to ensure reproducibility.\n",
    "\n",
    "A fundamental tenet of the ML workflow is splitting your data into training and testing sets. This involves deliberately withholding some datapoints from the model training in order to evaluate the performance of your model on 'unseen' data.\n",
    "\n",
    "It is vitally important to be able to reproducibly split your data across different training runs. For a few main reasons:\n",
    "- So you can use the same test datapoints to effectively compare the performance of different model candidates\n",
    "- To control as many variables as possible to help troubleshoot performance issues\n",
    "- To ensure that you, or your colleagues, can reproduce your model exactly\n",
    "\n",
    "**How you split your data can have a big effect on the perceived model performance**\n",
    "\n",
    "If you split your data 'randomly' there is a statistical chance that more outliers end up in the test set than the training set. As your model won't see many outliers during training, it will perform poorly on the test set when predicting 'outlier' values. \n",
    "\n",
    "Now imagine you randomly split the data again and the outliers now all reside in the training set and none in the test set. It is likely that your 'model performance' will increase. This performance increase has nothing to do with the model, just the statistical properties of the training/test sets.\n",
    "\n",
    "It is important to control and understand the training and test splits when comparing different model candidates and across multiple training runs.\n",
    "\n",
    "**Sklearn train_test_split**\n",
    " \n",
    "Probably, the most common way to split your dataset is to use Sklearn's [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function.\n",
    "\n",
    "Out of the box, the `train_test_split` function will randomly split your data into a training set and a test set. Each time you run the function you will get a different split for your data. Not ideal for reproducibility.\n",
    "\n",
    "\"Ah!\" you say. \"I set the random seed so it is reproducible!\". \n",
    "\n",
    "Fair point. Setting random seeds is certainly an excellent idea and goes a long way to improve reproducibility. I would highly recommend setting random seeds for any functions which have non-deterministic outputs. \n",
    "\n",
    "**However, random seeds might not be enough to ensure reproducibility** \n",
    "\n",
    "In this post I will demonstrate that the `train_test_split` function is more sensitive than you might think, and explain why using a random seed does not always guarantee reproducibility, particularly if you need to retrain your model in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72889ace",
   "metadata": {},
   "source": [
    "## What is the problem with train_test_split?\n",
    "\n",
    "**Setting the random reed only guarantees reproducible splits if the underlying data does not change in any way**\n",
    "\n",
    "The `train_test_split` is not *deterministic*. \n",
    "\n",
    "The splits from `train_test_split` are sensitive to any new data added to the dataset as well as the *ordering* of the underlying data.\n",
    "\n",
    "If your dataset is shuffled or amended in any way, the data will be split completely differently. It cannot be guaranteed that the an individual datapoint will *always* be in the training set or *always* be in the test set. This means datapoints that were in the original training set might now end up in the test set and visa versa if the data was shuffled. \n",
    "\n",
    "**Therefore, for the same dataset, you can get different splits depending on how the rows in the dataset are ordered.** \n",
    "\n",
    "It is not a very robust solution. \n",
    "\n",
    "Even if one datapoint is removed, the order of two rows are switched, or a single datapoint is added you will get a *completely* different training and test split.\n",
    "\n",
    "This 'ultra sensitivity' to the data might be surprising and lead to unexpected model training results which can be hard to debug and reproduce if your dataset ordering changes.\n",
    "\n",
    "Let's demonstrate the issue with a simple demo.\n",
    "\n",
    "We will first download an example dataset from `skelearn.datasets` and create an 'index' column to uniquely identify each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a13c8945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  mean radius  mean texture  mean perimeter  mean area  \\\n",
       "0      0        17.99         10.38          122.80     1001.0   \n",
       "1      1        20.57         17.77          132.90     1326.0   \n",
       "2      2        19.69         21.25          130.00     1203.0   \n",
       "3      3        11.42         20.38           77.58      386.1   \n",
       "4      4        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   mean symmetry  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0         0.2419  ...         25.38          17.33           184.60   \n",
       "1         0.1812  ...         24.99          23.41           158.80   \n",
       "2         0.2069  ...         23.57          25.53           152.50   \n",
       "3         0.2597  ...         14.91          26.50            98.87   \n",
       "4         0.1809  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"from sklearn.datasets import load_breast_cancer\\n\\nimport pandas as pd\\n\\n# download an example dataset\\ndata = load_breast_cancer()\\ndf = pd.DataFrame(data[\\\"data\\\"], columns=data[\\\"feature_names\\\"])\\n\\n# create an 'index' column to use to uniquely identify each row\\ndf = df.reset_index(drop=False)\\n\\ndf.head()\";\n",
       "                var nbb_formatted_code = \"from sklearn.datasets import load_breast_cancer\\n\\nimport pandas as pd\\n\\n# download an example dataset\\ndata = load_breast_cancer()\\ndf = pd.DataFrame(data[\\\"data\\\"], columns=data[\\\"feature_names\\\"])\\n\\n# create an 'index' column to use to uniquely identify each row\\ndf = df.reset_index(drop=False)\\n\\ndf.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# download an example dataset\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data[\"data\"], columns=data[\"feature_names\"])\n",
    "\n",
    "# create an 'index' column to use to uniquely identify each row\n",
    "df = df.reset_index(drop=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a2c124",
   "metadata": {},
   "source": [
    "Now let's split the data using Sklearn's `train_test_split`, setting the random state (seed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1224c72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"from sklearn.model_selection import train_test_split\\n\\n# set parameters\\nTEST_RATIO = 0.1\\nSEED = 42\\n\\n# split into training and test using a random seed\\nx_train_skl, x_test_skl = train_test_split(df, test_size=TEST_RATIO, random_state=SEED)\\n\\n# shuffle the orginal dataframe\\ndf_shuffled = df.sample(frac=1)\\n\\n# split the shuffled dataframe using the same random seed\\nx_train_skl_shuffled, x_test_skl_shuffled = train_test_split(\\n    df_shuffled, test_size=TEST_RATIO, random_state=SEED\\n)\";\n",
       "                var nbb_formatted_code = \"from sklearn.model_selection import train_test_split\\n\\n# set parameters\\nTEST_RATIO = 0.1\\nSEED = 42\\n\\n# split into training and test using a random seed\\nx_train_skl, x_test_skl = train_test_split(df, test_size=TEST_RATIO, random_state=SEED)\\n\\n# shuffle the orginal dataframe\\ndf_shuffled = df.sample(frac=1)\\n\\n# split the shuffled dataframe using the same random seed\\nx_train_skl_shuffled, x_test_skl_shuffled = train_test_split(\\n    df_shuffled, test_size=TEST_RATIO, random_state=SEED\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_RATIO = 0.1\n",
    "SEED = 42\n",
    "\n",
    "# split into training and test using a random seed\n",
    "x_train_skl, x_test_skl = train_test_split(df, test_size=TEST_RATIO, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf9646c",
   "metadata": {},
   "source": [
    "Next, we shuffle the original dataframe and split the data again. We will still use the same random seed as before for consistency.\n",
    "\n",
    "Note that no new data has been added, we have just reordered the rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9e29d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the orginal dataframe\n",
    "df_shuffled = df.sample(frac=1)\n",
    "\n",
    "# split the shuffled dataframe using the same random seed\n",
    "x_train_skl_shuffled, x_test_skl_shuffled = train_test_split(\n",
    "    df_shuffled, test_size=TEST_RATIO, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329cab57",
   "metadata": {},
   "source": [
    "Ideally the rows contained in the `x_test_skl` and `x_test_skl_shuffled` test sets should be identical as we used the same random seed. \n",
    "\n",
    "However, when we compare the row ids contained in each test set, we notice they are different! Even though the random state (seed) was the same both times. Nothing in the data has changed, it was just shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0f860ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 30;\n",
       "                var nbb_unformatted_code = \"# compare the row ids included in the original test set vs shuffled test set\\n# should return True if identical rows are included in each test set\\nset(x_test_skl[\\\"index\\\"]) == set(x_test_skl_shuffled[\\\"index\\\"])\";\n",
       "                var nbb_formatted_code = \"# compare the row ids included in the original test set vs shuffled test set\\n# should return True if identical rows are included in each test set\\nset(x_test_skl[\\\"index\\\"]) == set(x_test_skl_shuffled[\\\"index\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare the row ids included in the original test set vs shuffled test set\n",
    "# should return True if identical rows are included in each test set\n",
    "set(x_test_skl[\"index\"]) == set(x_test_skl_shuffled[\"index\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4f4e6e",
   "metadata": {},
   "source": [
    "This highlights just how sensitive the `train_test_split` function is, even to ordering of the data.\n",
    "\n",
    "More importantly, if there was a change in the underlying data, such as a reordering, which resulted in different splits, it would be extremely difficult to reproduce the original data splits and debug model performance.\n",
    "\n",
    "I was certainly surprised by how sensitive this function was to something as trivial as the ordering of the underlying dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4c5c4",
   "metadata": {},
   "source": [
    "## What are the consequences of relying on a random seed when retraining the model with updated data?\n",
    "\n",
    "As mentioned previously, the results of your model can vary significantly depending on how the data was split.\n",
    "\n",
    "In the future, when you come to retrain your model with some updated data you want to be able to control as many variables as possible in order to effectively compare the accuracy and performance of each model.\n",
    "\n",
    "**You cannot guarantee reproducibility and transparency of splits with random seed**\n",
    "\n",
    "It is be risky relying on a random seed from a reproducibility point of view.\n",
    "\n",
    "The random seed only guarantees reproducibility when the dataset has not changed in any way.\n",
    "\n",
    "Can you be 100% sure the dataset has not changed between training runs? If a colleague has removed an outlier data point or if new rows have been added. Your data splits will be completely different to your original splits with no way to easily replicate the old data splits.\n",
    "\n",
    "You can use data versioning tools, such as [dvc](https://dvc.org/) to help keep track of changes, however, that doesn't prevent your data splits changing. It would be better to protect against split changes in your code.\n",
    "\n",
    "Additionally, the process of splitting by the `train_test_split` function is not very transparent. It is non-deterministic. This means the same datapoint can be split differently and it is not clear why it appears in the training set one time and in the testing set the next.\n",
    "\n",
    "The lack of transparency makes it hard to understand or predict which split the datapoint will be placed. Now or in the future.\n",
    "\n",
    "**Difficult to effectively compare models**\n",
    "\n",
    "When comparing models, we want to be able to control as many variables as possible. That should include which datapoints were used for training and testing.\n",
    "\n",
    "If your data splits are significantly different between runs you might observe considerable differences in performance. For example if you have a couple of 'outlier' datapoints  that were in your training set for the original training run, but are now in your test set, you model performance might 'decline' as it could not predict outlier values in the test set as well as before.\n",
    "\n",
    "**Difficult to debug**\n",
    "\n",
    "If you can't effectively compare models, it can make it hard to debug performance issues.\n",
    "\n",
    "Imagine you add some new data points to your dataset and retrain your model, but the performance of the model drops.\n",
    "\n",
    "If you have used `train_test_split` with the random seed set, you will have completely different data splits as the underlying data has changed. It would be difficult to understand whether the model performance decline was due to the quality of the new data, or, as highlighted in the previous point, it was just because the data was split differently. So the model performance decline was due to statistical variation in the way the data was split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f8bd6",
   "metadata": {},
   "source": [
    "## When might train_test_split not be appropriate?\n",
    "\n",
    "**If you need to retrain your model in the future on the original data + new data**\n",
    "\n",
    "As demonstrated, any underlying change to your existing data, be it reordering or even adding one additional datapoint will cause completely different data splits. Your original data splits will not be reproducible.\n",
    "\n",
    "If you are retraining the model with a *completely* new dataset it isn't a problem as obviously all the training and test datapoints will be different.\n",
    "\n",
    "But if you are training again with a dataset that includes your original datapoints, ideally you should be able to replicate their original data splits during the new training run. Even with the random seed set, `train_test_split` will not guarantee this. \n",
    "\n",
    "\n",
    "**If you are sampling or retrieving your source data from an evolving data source**\n",
    "\n",
    "In an ideal situation, you should have full control of your source dataset, however, sometimes this is not the case.\n",
    "\n",
    "For example, if you are using a table stored in BigQuery as the source which is used be many teams. You cannot guarantee the order of the rows returned by a query and new rows might be appended to the table in the meantime.\n",
    "\n",
    "Another example, would be if you are working with image data stored in a filesystem. If new images are added to your source folder you cannot guarantee the ordering of filepaths especially if new images are added.\n",
    "\n",
    "**If there is a possibility of data leakage**\n",
    "\n",
    "Imagine you are working with a dataset containing information about airline arrival times and you want to predict the chances of a plane arriving late. \n",
    "\n",
    "It is likely that rows of data at the same date will be correlated with each other. If we randomly split the data we risk data leakage. Therefore, we want a method to split the data such that datapoints at the same date only end up int the training set, or only end up in the test set.\n",
    "\n",
    "`train_test_split` does not allow for splitting the data based on a particular column value.\n",
    "  \n",
    "**If you have large datasets which do not fit in memory**\n",
    "\n",
    "If you need to distribute your data across many machines in order to parallelise data processing, using a non-deterministic method for splitting your training and test data can be problematic and difficult to ensure reproducibility.\n",
    "\n",
    "\n",
    "**If your experimentation or production code will be rewritten in another language**\n",
    "\n",
    "As the `train_test_split` is non-deterministic, the data splits will not be easily reproducible across languages. \n",
    "\n",
    "For example, you might want to compare the performance of your custom Python model to a model created using BigQuery's BQML which is defined using SQL. The splits from `train_test_split` Sklearn will not easily translate directly to SQL.\n",
    "\n",
    "It can also be common for teams to prototype models using Python, but then write their production systems in another language such as Java. To help the process of translating the prototype model into another language, ideally we should be able to split the data in the same way in both languages to ensure reproducibility and help debug any differences from the original model to the new model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce2b57",
   "metadata": {},
   "source": [
    "## The Solution: Hashing\n",
    "\n",
    "### What is hashing?\n",
    "\n",
    "> \"A hash function is any function that can be used to map data of arbitrary size to fixed-size values\" [Wikipedia](https://en.wikipedia.org/wiki/Hash_function)\n",
    "\n",
    "There are many different hashing algorithms, but essentially they allow you to reproducibly convert an input into an arbitrary value.\n",
    "\n",
    "The output of the hashing function is deterministic -- it will always be the same for the same input.\n",
    "\n",
    "\n",
    "### How does it work for splitting data reproducibly?\n",
    "\n",
    "In the context of data splitting, we can use hashing to reliably assign splits to individual datapoints. As this is a deterministic process, we can ensure that the datapoint is always assigned to the same split which aids reproducibility.\n",
    "\n",
    "The process works as follows:\n",
    "- Use a unique identifier for the datapoint (e.g. an ID or by concatinating multiple columns) and use a hashing algorithm to convert it to an arbitrary integer. Each unique datapoint will have a unique output from the hashing function.\n",
    "- Use a [modulo operation](https://en.wikipedia.org/wiki/Modulo_operation) to arbitrarily split the data into 'buckets'\n",
    "- Select all datapoints in a subset of buckets to be the training set and the rest to be in the test set\n",
    "\n",
    "```\n",
    "# pseudo code\n",
    "id = \"0001\"\n",
    "hash_value = hash(id)\n",
    "bucket = hash_value % 10\n",
    "\n",
    "if bucket < 9:\n",
    "   train_set.append(id)\n",
    "else:\n",
    "   test_set.append(id)\n",
    "```\n",
    "\n",
    "\n",
    "### Reasons to use hashing\n",
    "\n",
    "**Deterministic**\n",
    "\n",
    "Hashing is robust to underlying changes in the data, unlike `train_test_split`.\n",
    "\n",
    "Using this method, an individual datapoint will *always* be assigned to the same bucket. The data split is now independent of the rest of the dataset. If the data is reordered or new data is added, the original \n",
    "\n",
    "**Improves development and reduces chances of human error**\n",
    "\n",
    "When working on models in parallel with colleagues, it is very easy to accidentally forget to use random seeds or even use different random seeds. This leaves you open the risk of human error.\n",
    "\n",
    "Using the same hashing algorithm removes the need to control reproducibility explicitly in your code  \n",
    "\n",
    "**Consistent splitting across raw and preprocessed data**\n",
    "\n",
    "Can help with experimentation if you can ensure the same splits across different preprocessed datasets.\n",
    "\n",
    "\n",
    "\n",
    "### Farmhash\n",
    "\n",
    "there are many hashing algorithms. example in sklearn, however, farmhash is another alternative which is easy to use and also supported (recommended) by BigQuery\n",
    "\n",
    "we want lightweight...go through characteristics we want\n",
    "\n",
    "why use fingerprint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import farmhash\n",
    "\n",
    "hashed_value = farmhash.fingerprint64(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7a74469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13009744463427800296\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# FARMHASH SIMPLE DEMO\\n# show single value hashing results\\n\\nimport farmhash\\n\\nhashed_value = farmhash.fingerprint64(\\\"hello\\\")\\nprint(hashed_value)\";\n",
       "                var nbb_formatted_code = \"# FARMHASH SIMPLE DEMO\\n# show single value hashing results\\n\\nimport farmhash\\n\\nhashed_value = farmhash.fingerprint64(\\\"hello\\\")\\nprint(hashed_value)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FARMHASH SIMPLE DEMO\n",
    "# show single value hashing results\n",
    "\n",
    "import farmhash\n",
    "\n",
    "hashed_value = farmhash.fingerprint64(\"hello\")\n",
    "print(hashed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1aba99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# assign a bucket using the modulus operation\\nhashed_value % 10\";\n",
       "                var nbb_formatted_code = \"# assign a bucket using the modulus operation\\nhashed_value % 10\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# assign a bucket using the modulus operation\n",
    "hashed_value % 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f7596",
   "metadata": {},
   "source": [
    "Therefore, our \"hello\" value would be assigned to bucket 6 of 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819f6cba",
   "metadata": {},
   "source": [
    "## Splitting the data using Farmhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61642fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# DEMO\\n# show distribution\";\n",
       "                var nbb_formatted_code = \"# DEMO\\n# show distribution\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DEMO\n",
    "# show distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26040d8d",
   "metadata": {},
   "source": [
    "### Considerations\n",
    "\n",
    "- Will not produce exactly 10% of the dataset --> less of a problem for large datasets\n",
    "- Which column should you use to hash?\n",
    "- How many buckets to chose?\n",
    "\n",
    "\n",
    "**Platform cross-compatibility quirks**\n",
    "\n",
    "Having said that hashing is consistent across platforms. That is true to an extent.\n",
    "\n",
    "While researching for this article, I tested to see if the dataset splits from my Python program were identical to a dataset stored in [BigQuery using SQL instead](https://towardsdatascience.com/ml-design-pattern-5-repeatable-sampling-c0ccb2889f39).\n",
    "\n",
    "To my surprise, I found some edge cases where the results differed.\n",
    "\n",
    "After a lot of head scratching, I came across a two quirks of BigQuery which were causing the issue:\n",
    "1. [BigQuery does not support unsigned integers](https://stackoverflow.com/questions/51892989/how-does-bigquerys-farm-fingerprint-represent-a-64-bit-unsigned-int)\n",
    "2. BigQuery, [C and C++ have different implementations for the modulus operation](https://stackoverflow.com/questions/3883004/the-modulo-operation-on-negative-numbers-in-python) compared to Python when it comes to negative numbers\n",
    "\n",
    "The `farmhash.fingerprint64` method returns an *[unsigned](https://stackoverflow.com/questions/247873/signed-versus-unsigned-integers)* integer. Whereas the BigQuery `FARM_FINGERPRINT` function returns an `INT64` datatype. Therefore, BigQuery converts the farmhash output to a *signed* integer. The value of signed integer might be different to the original unsigned integer value and it might be negative.\n",
    "\n",
    "Ok, can't we just convert our Python farmhash value to a signed integer?\n",
    "\n",
    "Well, this leads to the second problem. In order to split the data into different buckets, we used the modulus operation. [Python and C (and BigQuery) have different implementations of the modulus operator](https://stackoverflow.com/questions/1907565/c-and-python-different-behaviour-of-the-modulo-operation#:~:text=Both%20variants%20are%20correct%2C%20however,same%20result%20as%20in%20Python.) which affects the results for negative numbers.\n",
    "\n",
    "Therefore, if our signed integer is negative, the bucket it is assigned will differ depending on if we calculated it using Python's modulus operator (`%`) or BigQuery's `MOD` function. \n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "# in python\n",
    "-1%10 = 9\n",
    "\n",
    "# in BigQuery\n",
    "MOD(-1,10) = -1\n",
    "```\n",
    "\n",
    "Does this mean hashing is not platform independent? \n",
    "\n",
    "Well, not really, just that you need to check how unsigned integers are treated and the how modulus of negative numbers is implemented.\n",
    "\n",
    "We can fix our Python implementation to protect against these issues by converting the unsigned output of farmhash to a signed int and using a custom 'C-like' implementation of the modulus function.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def signed_hash_value(value:Any) -> int:\n",
    "    \"\"\"Convert unsigned hashed value to a signed int\"\"\"\n",
    "    hashed_value = farmhash.fingerprint64(str(value))\n",
    "    return np.uint64(hashed_value).astype(\"int64\")\n",
    "    \n",
    "def c_mod(a:int, b:int)->int:\n",
    "    \"\"\"Modulus function implemented to behave like 'C' languages\"\"\"\n",
    "    res = a % b\n",
    "    if a < 0:\n",
    "        res -= b\n",
    "    return res\n",
    "\n",
    "# this will give the same results in Python and BigQuery (and C-languages)\n",
    "assigned_bucket = abs(c_mod(signed_hash_value,10))\n",
    "```\n",
    "\n",
    "[Note: the order of the abs and mod functions is important](https://mentin.medium.com/be-careful-with-abs-function-8e91c78715d5)\n",
    "\n",
    "https://stackoverflow.com/questions/63341637/python-vs-bigquery-farmhash-sometimes-do-not-equal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9615921",
   "metadata": {},
   "source": [
    "### Alternatives to hashing\n",
    "\n",
    "For completeness, here are two other common and more explicit approaches for ensuring consistent train/test splitting. \n",
    "\n",
    "**Create an additional column in your data**\n",
    "\n",
    "You could use `train_test_split` (or another random splitting method) to initially define the splits. Then create an additional column in your dataset to explicitly record whether the datapoint should be included for training or test (or specify the fold for K-Fold validation).\n",
    "\n",
    "Here is an [example implementation by Abhishek Thakur](https://github.com/abhishekkrthakur/mlframework/blob/master/src/create_folds.py) who uses it to define the 'folds' for cross validation.\n",
    "\n",
    "This will ensure your splits are 'remembered' between training runs as they are explicitly defined. \n",
    "\n",
    "On the positive side, it is very transparent which datapoints belong to each split. However, a downside is that it increases the total size of your dataset which may not be sustainable for very large datasets. Additionally, if you do not have full control of the dataset (e.g. shared database table) you may not be able to add columns to the original schema.\n",
    "\n",
    "**Save your training and test data to different files**\n",
    "\n",
    "Another common approach is to store your training data and test data to individual files after splitting the data for the first time. For example, into files called `train.csv` and `test.csv`. If the data is too large for individual files you could also save multiple files into folders named `train` and `test` respectively.\n",
    "\n",
    "This can be a valid approach. However, sometimes it may not be feasible to make a copy of all your data and save into individual files. For example if the dataset size is very large or you don't have the permissions to make a copy from the original source.\n",
    "\n",
    "The hashing approach can compute the deterministic data splits reproducibly on the fly and in-memory, preventing the need to copy data into individual files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf8d817",
   "metadata": {},
   "source": [
    "### Other use cases for hashing\n",
    "\n",
    "Hashing is a very powerful technique which is used extensively in machine learning systems for other use cases.\n",
    "\n",
    "**Data obfuscation**\n",
    "\n",
    "Hashing can be used to obfuscate personal or sensitive data. For example, converting an email, company name or post code into a hashed value string. This encoded string still holds important categorical information that can can be used to train models and in exploratory data analysis to understand more about the data without leaking personally identifiable data.\n",
    "\n",
    "**Categorical variable encoding**\n",
    "\n",
    "Hashing is also used for feature engineering, and can be particularly useful for dealing with unseen data in the test set.\n",
    "\n",
    "See ML Design patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba1533",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "ML is experimental and iterative, you need to be able to keep as many variables constant as possible. That includes which datapoints are used for training and which are used for testing.\n",
    "\n",
    "- The crux, why is this important?: For reproducibility you want to ensure the data that was used to train a model remains consistent when you go to retrain it. This helps comparison of performance and debugging.\n",
    "- Particularly if you need to retrain your model in the future (i.e. it is not just a one off)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b60f1b8",
   "metadata": {},
   "source": [
    "## References and resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce4668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "017fc683",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00768709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2818e956",
   "metadata": {},
   "source": [
    "### More indepth code on BigQuery unsigned int debate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc2c7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee7fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15253d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fbd333e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  mean radius  mean texture  mean perimeter  mean area  \\\n",
       "0      0        17.99         10.38          122.80     1001.0   \n",
       "1      1        20.57         17.77          132.90     1326.0   \n",
       "2      2        19.69         21.25          130.00     1203.0   \n",
       "3      3        11.42         20.38           77.58      386.1   \n",
       "4      4        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   mean symmetry  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0         0.2419  ...         25.38          17.33           184.60   \n",
       "1         0.1812  ...         24.99          23.41           158.80   \n",
       "2         0.2069  ...         23.57          25.53           152.50   \n",
       "3         0.2597  ...         14.91          26.50            98.87   \n",
       "4         0.1809  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"from typing import Any\\n\\nimport farmhash\\nimport seaborn as sns\\nfrom sklearn.datasets import load_breast_cancer\\n\\nimport pandas as pd\\n\\n# set parameters\\nTEST_RATIO = 0.1\\nSEED = 42\\nBUCKETS = 10\\n\\n# download an example dataset\\ndata = load_breast_cancer()\\ndf = pd.DataFrame(data[\\\"data\\\"], columns=data[\\\"feature_names\\\"])\\n\\n# create an 'index' column to use to uniquely identify each row\\ndf = df.reset_index(drop=False)\\n\\ndf.head()\";\n",
       "                var nbb_formatted_code = \"from typing import Any\\n\\nimport farmhash\\nimport seaborn as sns\\nfrom sklearn.datasets import load_breast_cancer\\n\\nimport pandas as pd\\n\\n# set parameters\\nTEST_RATIO = 0.1\\nSEED = 42\\nBUCKETS = 10\\n\\n# download an example dataset\\ndata = load_breast_cancer()\\ndf = pd.DataFrame(data[\\\"data\\\"], columns=data[\\\"feature_names\\\"])\\n\\n# create an 'index' column to use to uniquely identify each row\\ndf = df.reset_index(drop=False)\\n\\ndf.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "import farmhash\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# set parameters\n",
    "TEST_RATIO = 0.1\n",
    "SEED = 42\n",
    "BUCKETS = 10\n",
    "\n",
    "# download an example dataset\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data[\"data\"], columns=data[\"feature_names\"])\n",
    "\n",
    "# create an 'index' column to use to uniquely identify each row\n",
    "df = df.reset_index(drop=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d428a7b7",
   "metadata": {},
   "source": [
    "**Sklearn `train_test_split` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faea099b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"from sklearn.model_selection import train_test_split\\n\\n# split into training and test using a random seed\\nx_train_skl, x_test_skl = train_test_split(df, test_size=TEST_RATIO, random_state=SEED)\\n\\n# shuffle the orginal dataframe\\ndf_shuffled = df.sample(frac=1)\\n\\n# split the shuffled dataframe using the same random seed\\nx_train_skl_shuffled, x_test_skl_shuffled = train_test_split(\\n    df_shuffled, test_size=TEST_RATIO, random_state=SEED\\n)\";\n",
       "                var nbb_formatted_code = \"from sklearn.model_selection import train_test_split\\n\\n# split into training and test using a random seed\\nx_train_skl, x_test_skl = train_test_split(df, test_size=TEST_RATIO, random_state=SEED)\\n\\n# shuffle the orginal dataframe\\ndf_shuffled = df.sample(frac=1)\\n\\n# split the shuffled dataframe using the same random seed\\nx_train_skl_shuffled, x_test_skl_shuffled = train_test_split(\\n    df_shuffled, test_size=TEST_RATIO, random_state=SEED\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split into training and test using a random seed\n",
    "x_train_skl, x_test_skl = train_test_split(df, test_size=TEST_RATIO, random_state=SEED)\n",
    "\n",
    "# shuffle the orginal dataframe\n",
    "df_shuffled = df.sample(frac=1)\n",
    "\n",
    "# split the shuffled dataframe using the same random seed\n",
    "x_train_skl_shuffled, x_test_skl_shuffled = train_test_split(\n",
    "    df_shuffled, test_size=TEST_RATIO, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b363d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7866f65e",
   "metadata": {},
   "source": [
    "Let's compare the test sets using the row index which uniquely identifies each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "417c3104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"# compare the row ids included in each test set\\nset(x_test_skl[\\\"index\\\"]) == set(x_test_skl_shuffled[\\\"index\\\"])\";\n",
       "                var nbb_formatted_code = \"# compare the row ids included in each test set\\nset(x_test_skl[\\\"index\\\"]) == set(x_test_skl_shuffled[\\\"index\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare the row ids included in each test set\n",
    "set(x_test_skl[\"index\"]) == set(x_test_skl_shuffled[\"index\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49173c6c",
   "metadata": {},
   "source": [
    "The rows included in each test set are not the same. Even though we set the random seed to be the same.\n",
    "\n",
    "This highlights how the sklearn `train_test_split` is sensitive to changes and ordering of the data.\n",
    "\n",
    "If you can't guarantee the order of the data will be the same each time, or if you are retraining on a dataset which contains the original data plus some new data, your data splits will be different even if you set the random seed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c4fd4",
   "metadata": {},
   "source": [
    "**Hashing method**\n",
    "\n",
    "For the hashing method we will need to create our own functions.\n",
    "\n",
    "We need to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe15218b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"def hash_value(value: Any) -> int:\\n    \\\"\\\"\\\"convert a value into a hashed value\\\"\\\"\\\"\\n    return farmhash.fingerprint64(str(value))\\n\\n\\ndef convert_hash_to_bucket(hashed_value: int, total_buckets: int) -> int:\\n    \\\"\\\"\\\"assign a bucket based off the hash value\\\"\\\"\\\"\\n    return abs(hashed_value % total_buckets)\\n\\n\\ndef test_set_check(bucket: int):\\n    \\\"\\\"\\\"check if the bucket should be included in the test set\\\"\\\"\\\"\\n    return bucket < TEST_RATIO * BUCKETS\\n\\n\\ndef assign_hash_bucket(value: Any) -> int:\\n    \\\"\\\"\\\"assign a bucket to an input value using hashing algorithm\\\"\\\"\\\"\\n    hashed_value = hash_value(value)\\n    bucket = convert_hash_to_bucket(hashed_value, total_buckets=BUCKETS)\\n    return bucket\\n\\n\\ndef hash_train_test_split(\\n    df: pd.DataFrame, split_col: str, approx_test_ratio: float\\n) -> tuple[pd.DataFrame, pd.DataFrame]:\\n    \\\"\\\"\\\"Split the data into a training and test set based of a specific column\\n\\n    Args:\\n        df (pd.DataFrame): original dataset\\n        split_col (str): name of the column to use for hashing algo\\n        approx_test_ratio (float): between 0-1. This is an approximate ratio\\n           as the hashing algo will not necessarily provide a uniform bucket\\n           distribution\\n\\n    Returns:\\n        tuple: Two dataframes, first is the training set and second is the\\n           test set\\n    \\\"\\\"\\\"\\n    df[\\\"buckets\\\"] = df[split_col].apply(assign_hash_bucket)\\n    in_test_set = df[\\\"buckets\\\"].apply(test_set_check)\\n    return df[~in_test_set], df[in_test_set]\";\n",
       "                var nbb_formatted_code = \"def hash_value(value: Any) -> int:\\n    \\\"\\\"\\\"convert a value into a hashed value\\\"\\\"\\\"\\n    return farmhash.fingerprint64(str(value))\\n\\n\\ndef convert_hash_to_bucket(hashed_value: int, total_buckets: int) -> int:\\n    \\\"\\\"\\\"assign a bucket based off the hash value\\\"\\\"\\\"\\n    return abs(hashed_value % total_buckets)\\n\\n\\ndef test_set_check(bucket: int):\\n    \\\"\\\"\\\"check if the bucket should be included in the test set\\\"\\\"\\\"\\n    return bucket < TEST_RATIO * BUCKETS\\n\\n\\ndef assign_hash_bucket(value: Any) -> int:\\n    \\\"\\\"\\\"assign a bucket to an input value using hashing algorithm\\\"\\\"\\\"\\n    hashed_value = hash_value(value)\\n    bucket = convert_hash_to_bucket(hashed_value, total_buckets=BUCKETS)\\n    return bucket\\n\\n\\ndef hash_train_test_split(\\n    df: pd.DataFrame, split_col: str, approx_test_ratio: float\\n) -> tuple[pd.DataFrame, pd.DataFrame]:\\n    \\\"\\\"\\\"Split the data into a training and test set based of a specific column\\n\\n    Args:\\n        df (pd.DataFrame): original dataset\\n        split_col (str): name of the column to use for hashing algo\\n        approx_test_ratio (float): between 0-1. This is an approximate ratio\\n           as the hashing algo will not necessarily provide a uniform bucket\\n           distribution\\n\\n    Returns:\\n        tuple: Two dataframes, first is the training set and second is the\\n           test set\\n    \\\"\\\"\\\"\\n    df[\\\"buckets\\\"] = df[split_col].apply(assign_hash_bucket)\\n    in_test_set = df[\\\"buckets\\\"].apply(test_set_check)\\n    return df[~in_test_set], df[in_test_set]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def hash_value(value: Any) -> int:\n",
    "    \"\"\"convert a value into a hashed value\"\"\"\n",
    "    return farmhash.fingerprint64(str(value))\n",
    "\n",
    "\n",
    "def convert_hash_to_bucket(hashed_value: int, total_buckets: int) -> int:\n",
    "    \"\"\"assign a bucket based off the hash value\"\"\"\n",
    "    return abs(hashed_value % total_buckets)\n",
    "\n",
    "\n",
    "def test_set_check(bucket: int):\n",
    "    \"\"\"check if the bucket should be included in the test set\"\"\"\n",
    "    return bucket < TEST_RATIO * BUCKETS\n",
    "\n",
    "\n",
    "def assign_hash_bucket(value: Any) -> int:\n",
    "    \"\"\"assign a bucket to an input value using hashing algorithm\"\"\"\n",
    "    hashed_value = hash_value(value)\n",
    "    bucket = convert_hash_to_bucket(hashed_value, total_buckets=BUCKETS)\n",
    "    return bucket\n",
    "\n",
    "\n",
    "def hash_train_test_split(\n",
    "    df: pd.DataFrame, split_col: str, approx_test_ratio: float\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Split the data into a training and test set based of a specific column\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): original dataset\n",
    "        split_col (str): name of the column to use for hashing algo\n",
    "        approx_test_ratio (float): between 0-1. This is an approximate ratio\n",
    "           as the hashing algo will not necessarily provide a uniform bucket\n",
    "           distribution\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two dataframes, first is the training set and second is the\n",
    "           test set\n",
    "    \"\"\"\n",
    "    df[\"buckets\"] = df[split_col].apply(assign_hash_bucket)\n",
    "    in_test_set = df[\"buckets\"].apply(test_set_check)\n",
    "    return df[~in_test_set], df[in_test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7352f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"# create a training and test set from original dataset using hashing method\\nx_train_hash, x_test_hash = hash_train_test_split(\\n    df, split_col=\\\"index\\\", approx_test_ratio=TEST_RATIO\\n)\\n\\n\\n# create a training and test set from shuffled dataset using hashing method\\nx_train_hash, x_test_hash_shuffled = hash_train_test_split(\\n    df_shuffled, split_col=\\\"index\\\", approx_test_ratio=TEST_RATIO\\n)\";\n",
       "                var nbb_formatted_code = \"# create a training and test set from original dataset using hashing method\\nx_train_hash, x_test_hash = hash_train_test_split(\\n    df, split_col=\\\"index\\\", approx_test_ratio=TEST_RATIO\\n)\\n\\n\\n# create a training and test set from shuffled dataset using hashing method\\nx_train_hash, x_test_hash_shuffled = hash_train_test_split(\\n    df_shuffled, split_col=\\\"index\\\", approx_test_ratio=TEST_RATIO\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a training and test set from original dataset using hashing method\n",
    "x_train_hash, x_test_hash = hash_train_test_split(\n",
    "    df, split_col=\"index\", approx_test_ratio=TEST_RATIO\n",
    ")\n",
    "\n",
    "\n",
    "# create a training and test set from shuffled dataset using hashing method\n",
    "x_train_hash, x_test_hash_shuffled = hash_train_test_split(\n",
    "    df_shuffled, split_col=\"index\", approx_test_ratio=TEST_RATIO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67b1f244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"# compare the row ids included in each test set\\nset(x_test_hash[\\\"index\\\"]) == set(x_test_hash_shuffled[\\\"index\\\"])\";\n",
       "                var nbb_formatted_code = \"# compare the row ids included in each test set\\nset(x_test_hash[\\\"index\\\"]) == set(x_test_hash_shuffled[\\\"index\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare the row ids included in each test set\n",
    "set(x_test_hash[\"index\"]) == set(x_test_hash_shuffled[\"index\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f81211",
   "metadata": {},
   "source": [
    "Problem solved! Even though the underlying dataframe has been shuffled, the same row ids appear in the test dataset regardless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed8cd45",
   "metadata": {},
   "source": [
    "Note, however, with the hashing method your data will not necessarily be split exactly by the ratio you specify. The larger your dataset, the less of an issue this will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b3951dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 46)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"total_records = len(df)\\ntarget_records = int(total_records * TEST_RATIO)\\nhash_test_set_records = len(x_test_hash)\\n\\ntarget_records, hash_test_set_records\";\n",
       "                var nbb_formatted_code = \"total_records = len(df)\\ntarget_records = int(total_records * TEST_RATIO)\\nhash_test_set_records = len(x_test_hash)\\n\\ntarget_records, hash_test_set_records\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_records = len(df)\n",
    "target_records = int(total_records * TEST_RATIO)\n",
    "hash_test_set_records = len(x_test_hash)\n",
    "\n",
    "target_records, hash_test_set_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77201c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"values = [farmhash.fingerprint64(str(i)) % 10 for i in range(1_000_000_0)]\\nsns.histplot(\\n    data=values,\\n    color=\\\"skyblue\\\",\\n    binwidth=1,\\n)\";\n",
       "                var nbb_formatted_code = \"values = [farmhash.fingerprint64(str(i)) % 10 for i in range(1_000_000_0)]\\nsns.histplot(\\n    data=values,\\n    color=\\\"skyblue\\\",\\n    binwidth=1,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "values = [farmhash.fingerprint64(str(i)) % 10 for i in range(1_000_000_0)]\n",
    "sns.histplot(\n",
    "    data=values,\n",
    "    color=\"skyblue\",\n",
    "    binwidth=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "447d0a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({5: 1001297,\n",
       "         9: 998108,\n",
       "         1: 998962,\n",
       "         3: 1000155,\n",
       "         7: 999531,\n",
       "         8: 999567,\n",
       "         2: 1000679,\n",
       "         6: 1000185,\n",
       "         0: 1000657,\n",
       "         4: 1000859})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"from collections import Counter\\n\\nCounter(values)\";\n",
       "                var nbb_formatted_code = \"from collections import Counter\\n\\nCounter(values)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "958b81e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"import farmhash\\n\\n\\ndir(farmhash)\\n\\nabs(farmhash.fingerprint64(\\\"abc\\\") % 10)\";\n",
       "                var nbb_formatted_code = \"import farmhash\\n\\n\\ndir(farmhash)\\n\\nabs(farmhash.fingerprint64(\\\"abc\\\") % 10)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import farmhash\n",
    "\n",
    "dir(farmhash)\n",
    "\n",
    "abs(farmhash.fingerprint64(\"abc\") % 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ded10a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"import numpy as np\\n\\nn = np.uint64(farmhash.fingerprint64(\\\"1footrue\\\")).astype(\\\"int64\\\")\\nn % 10\";\n",
       "                var nbb_formatted_code = \"import numpy as np\\n\\nn = np.uint64(farmhash.fingerprint64(\\\"1footrue\\\")).astype(\\\"int64\\\")\\nn % 10\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = np.uint64(farmhash.fingerprint64(\"1footrue\")).astype(\"int64\")\n",
    "n % 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2b5c719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"def c_mod(a, b):\\n    res = a % b\\n    if a < 0:\\n        res -= b\\n    return res\\n\\n\\nc_mod(n, 10)\";\n",
       "                var nbb_formatted_code = \"def c_mod(a, b):\\n    res = a % b\\n    if a < 0:\\n        res -= b\\n    return res\\n\\n\\nc_mod(n, 10)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def c_mod(a, b):\n",
    "    res = a % b\n",
    "    if a < 0:\n",
    "        res -= b\n",
    "    return res\n",
    "\n",
    "\n",
    "c_mod(n, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20592d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"abs(c_mod(-1541654101129638711, 10))\";\n",
       "                var nbb_formatted_code = \"abs(c_mod(-1541654101129638711, 10))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "abs(c_mod(-1541654101129638711, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f6e94a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"abs(-1541654101129638711) % 10\";\n",
       "                var nbb_formatted_code = \"abs(-1541654101129638711) % 10\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "abs(-1541654101129638711) % 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e59ab80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"s = -1541654101129638711\\nabs(s % 10)\";\n",
       "                var nbb_formatted_code = \"s = -1541654101129638711\\nabs(s % 10)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = -1541654101129638711\n",
    "abs(s % 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc0cd715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 26;\n",
       "                var nbb_unformatted_code = \"t = np.uint64(farmhash.fingerprint64(\\\"1footrue\\\")).astype(\\\"int64\\\")\\nabs(t % 10)\";\n",
       "                var nbb_formatted_code = \"t = np.uint64(farmhash.fingerprint64(\\\"1footrue\\\")).astype(\\\"int64\\\")\\nabs(t % 10)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = np.uint64(farmhash.fingerprint64(\"1footrue\")).astype(\"int64\")\n",
    "abs(t % 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a15373b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_unformatted_code = \"-11 % 10\";\n",
       "                var nbb_formatted_code = \"-11 % 10\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "-11 % 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d5b8f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"-1541654101129638711 % 10\";\n",
       "                var nbb_formatted_code = \"-1541654101129638711 % 10\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "-1541654101129638711 % 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a831e9",
   "metadata": {},
   "source": [
    "SHOW MORE INFO ON HOW THE BUCKET DISTRIBUTIONS HAPPEN - E.G. HISTOGRAM PLOTS?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386d0cb",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "**Hashing**\n",
    "- [ML Design Pattern: Repeatable sampling](https://towardsdatascience.com/ml-design-pattern-5-repeatable-sampling-c0ccb2889f39) (inspiration for this post)\n",
    "- [Hash your data before you create the train-test split](https://www.bi-kring.nl/192-data-science/1340-reusing-data-for-ml-hash-your-data-before-you-create-the-train-test-split)\n",
    "\n",
    "- [Farmhash Description](https://github.com/google/farmhash/blob/master/Understanding_Hash_Functions)\n",
    "- [Hands on ML - different implementation](https://www.danli.org/2021/06/06/hands-on-machine-learning/) also [see notebook](https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb)\n",
    "- [Python Farmhash implementation](https://github.com/veelion/python-farmhash)\n",
    "\n",
    "\n",
    "**Differences between Python and BigQuery**\n",
    "- [Farmhash vs BigQuery implementation, GitHub issue](https://github.com/lovell/farmhash/issues/26)\n",
    "- [StackOverflow question: difference in results between BigQuery and Python](https://stackoverflow.com/questions/63341637/python-vs-bigquery-farmhash-sometimes-do-not-equal)\n",
    "- [BigQuery returns signed integers from FARM_FINGERPRINT function](https://stackoverflow.com/questions/51892989/how-does-bigquerys-farm-fingerprint-represent-a-64-bit-unsigned-int)\n",
    "- [Be careful about operation order in BigQuery](https://mentin.medium.com/be-careful-with-abs-function-8e91c78715d5)\n",
    "- [Reproducibly sampling in BigQuery](https://towardsdatascience.com/advanced-random-sampling-in-bigquery-sql-7d4483b580bb)\n",
    "\n",
    "\n",
    "- [Google docs: Considerations for Hashing](https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/randomization)\n",
    "\n",
    "**Reproducibility**\n",
    "https://towardsdatascience.com/reproducibility-in-data-science-c2ac9e689339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa60f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c20e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
