{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "101695fb",
   "metadata": {},
   "source": [
    "# ML Design Pattern: Repeatable Splitting\n",
    "\n",
    "\n",
    "## Reproducible ML: Maybe you shouldn't be using train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c5461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "# ! pip install farmhash pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dad65dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d64e85ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e68f55c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 669\n",
      "Test set: 59\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# import the libraries\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom zlib import crc32\\n\\n\\n# generate the first DataFrame\\nX_1 = pd.DataFrame(data={\\\"variable\\\": np.random.normal(size=1000)})\\n\\n# apply the train-test split\\nX_1_train, X_1_test = train_test_split(X_1, test_size=0.2, random_state=42)\\n\\n# add new observations to the DataFrame\\nX_2 = pd.concat(\\n    [X_1, pd.DataFrame(data={\\\"variable\\\": np.random.normal(size=500)})]\\n).reset_index(drop=True)\\n\\n# again, apply the train-test split to the updated DataFrame\\nX_2_train, X_2_test = train_test_split(X_2, test_size=0.2, random_state=42)\\n\\n# see what is the overlap of indices\\nprint(f\\\"Train set: {len(set(X_1_train.index).intersection(set(X_2_train.index)))}\\\")\\nprint(f\\\"Test set: {len(set(X_1_test.index).intersection(set(X_2_test.index)))}\\\")\";\n",
       "                var nbb_formatted_code = \"# import the libraries\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom zlib import crc32\\n\\n\\n# generate the first DataFrame\\nX_1 = pd.DataFrame(data={\\\"variable\\\": np.random.normal(size=1000)})\\n\\n# apply the train-test split\\nX_1_train, X_1_test = train_test_split(X_1, test_size=0.2, random_state=42)\\n\\n# add new observations to the DataFrame\\nX_2 = pd.concat(\\n    [X_1, pd.DataFrame(data={\\\"variable\\\": np.random.normal(size=500)})]\\n).reset_index(drop=True)\\n\\n# again, apply the train-test split to the updated DataFrame\\nX_2_train, X_2_test = train_test_split(X_2, test_size=0.2, random_state=42)\\n\\n# see what is the overlap of indices\\nprint(f\\\"Train set: {len(set(X_1_train.index).intersection(set(X_2_train.index)))}\\\")\\nprint(f\\\"Test set: {len(set(X_1_test.index).intersection(set(X_2_test.index)))}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import the libraries\n",
    "import pandas as pd\n",
    "\n",
    "# generate the first DataFrame\n",
    "X_1 = pd.DataFrame(data={\"variable\": np.random.normal(size=1000)})\n",
    "\n",
    "# apply the train-test split\n",
    "X_1_train, X_1_test = train_test_split(X_1, test_size=0.2, random_state=42)\n",
    "\n",
    "# add new observations to the DataFrame\n",
    "X_2 = pd.concat(\n",
    "    [X_1, pd.DataFrame(data={\"variable\": np.random.normal(size=500)})]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# again, apply the train-test split to the updated DataFrame\n",
    "X_2_train, X_2_test = train_test_split(X_2, test_size=0.2, random_state=42)\n",
    "\n",
    "# see what is the overlap of indices\n",
    "print(f\"Train set: {len(set(X_1_train.index).intersection(set(X_2_train.index)))}\")\n",
    "print(f\"Test set: {len(set(X_1_test.index).intersection(set(X_2_test.index)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96a4493b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"import farmhash\";\n",
       "                var nbb_formatted_code = \"import farmhash\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import farmhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcc1d858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>airport</th>\n",
       "      <th>hash_airport</th>\n",
       "      <th>hash_id</th>\n",
       "      <th>airport_bucket_10</th>\n",
       "      <th>id_bucket_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>LBB</td>\n",
       "      <td>6290192273091124709</td>\n",
       "      <td>15198969275252572735</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>DTW</td>\n",
       "      <td>8485400555133853543</td>\n",
       "      <td>9304157803607034849</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ANC</td>\n",
       "      <td>3769719083775059508</td>\n",
       "      <td>6920640749119438759</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>MSO</td>\n",
       "      <td>3704558892068952737</td>\n",
       "      <td>11991475895402502921</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MSO</td>\n",
       "      <td>3704558892068952737</td>\n",
       "      <td>14526854387307260543</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id airport         hash_airport               hash_id  airport_bucket_10  \\\n",
       "0   0     LBB  6290192273091124709  15198969275252572735                  9   \n",
       "1   1     DTW  8485400555133853543   9304157803607034849                  3   \n",
       "2   2     ANC  3769719083775059508   6920640749119438759                  8   \n",
       "3   3     MSO  3704558892068952737  11991475895402502921                  7   \n",
       "4   4     MSO  3704558892068952737  14526854387307260543                  7   \n",
       "\n",
       "   id_bucket_10  \n",
       "0             5  \n",
       "1             9  \n",
       "2             9  \n",
       "3             1  \n",
       "4             3  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"airport = [\\\"DTW\\\", \\\"LBB\\\", \\\"SNA\\\", \\\"MSO\\\", \\\"ANC\\\", \\\"PIT\\\", \\\"PWM\\\", \\\"BNA\\\"]\\n\\nimport random\\n\\nSEED = 42\\nRECORDS = 100000\\nBUCKETS = 10\\n\\nrandom.seed(SEED)\\na = [(i, random.choice(airport)) for i in range(RECORDS)]\\n\\ndf = pd.DataFrame(a, columns=[\\\"id\\\", \\\"airport\\\"])\\n\\ndf[\\\"hash_airport\\\"] = df[\\\"airport\\\"].apply(lambda x: farmhash.hash64(x))\\ndf[\\\"hash_id\\\"] = df[\\\"id\\\"].apply(lambda x: farmhash.hash64(str(x)))\\n\\ndf[\\\"airport_bucket_10\\\"] = df[\\\"hash_airport\\\"].apply(lambda x: abs(x % BUCKETS))\\ndf[\\\"id_bucket_10\\\"] = df[\\\"hash_id\\\"].apply(lambda x: abs(x % BUCKETS))\\ndf.head()\";\n",
       "                var nbb_formatted_code = \"airport = [\\\"DTW\\\", \\\"LBB\\\", \\\"SNA\\\", \\\"MSO\\\", \\\"ANC\\\", \\\"PIT\\\", \\\"PWM\\\", \\\"BNA\\\"]\\n\\nimport random\\n\\nSEED = 42\\nRECORDS = 100000\\nBUCKETS = 10\\n\\nrandom.seed(SEED)\\na = [(i, random.choice(airport)) for i in range(RECORDS)]\\n\\ndf = pd.DataFrame(a, columns=[\\\"id\\\", \\\"airport\\\"])\\n\\ndf[\\\"hash_airport\\\"] = df[\\\"airport\\\"].apply(lambda x: farmhash.hash64(x))\\ndf[\\\"hash_id\\\"] = df[\\\"id\\\"].apply(lambda x: farmhash.hash64(str(x)))\\n\\ndf[\\\"airport_bucket_10\\\"] = df[\\\"hash_airport\\\"].apply(lambda x: abs(x % BUCKETS))\\ndf[\\\"id_bucket_10\\\"] = df[\\\"hash_id\\\"].apply(lambda x: abs(x % BUCKETS))\\ndf.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "airport = [\"DTW\", \"LBB\", \"SNA\", \"MSO\", \"ANC\", \"PIT\", \"PWM\", \"BNA\"]\n",
    "\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "RECORDS = 100000\n",
    "BUCKETS = 10\n",
    "\n",
    "random.seed(SEED)\n",
    "a = [(i, random.choice(airport)) for i in range(RECORDS)]\n",
    "\n",
    "df = pd.DataFrame(a, columns=[\"id\", \"airport\"])\n",
    "\n",
    "df[\"hash_airport\"] = df[\"airport\"].apply(lambda x: farmhash.hash64(x))\n",
    "df[\"hash_id\"] = df[\"id\"].apply(lambda x: farmhash.hash64(str(x)))\n",
    "\n",
    "df[\"airport_bucket_10\"] = df[\"hash_airport\"].apply(lambda x: abs(x % BUCKETS))\n",
    "df[\"id_bucket_10\"] = df[\"hash_id\"].apply(lambda x: abs(x % BUCKETS))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0756a459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected = 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "airport_bucket_10    37871\n",
       "id_bucket_10          9979\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"print(f\\\"Expected = {int(RECORDS*0.1)}\\\")\\ndf[df[[\\\"airport_bucket_10\\\", \\\"id_bucket_10\\\"]] >= 9][\\n    [\\\"airport_bucket_10\\\", \\\"id_bucket_10\\\"]\\n].count()\";\n",
       "                var nbb_formatted_code = \"print(f\\\"Expected = {int(RECORDS*0.1)}\\\")\\ndf[df[[\\\"airport_bucket_10\\\", \\\"id_bucket_10\\\"]] >= 9][\\n    [\\\"airport_bucket_10\\\", \\\"id_bucket_10\\\"]\\n].count()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Expected = {int(RECORDS*0.1)}\")\n",
    "df[df[[\"airport_bucket_10\", \"id_bucket_10\"]] >= 9][\n",
    "    [\"airport_bucket_10\", \"id_bucket_10\"]\n",
    "].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4820945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"import seaborn as sns\";\n",
       "                var nbb_formatted_code = \"import seaborn as sns\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "246e5316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='airport_bucket_10', ylabel='Count'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"sns.histplot(\\n    data=df,\\n    x=\\\"airport_bucket_10\\\",\\n    color=\\\"skyblue\\\",\\n    label=\\\"airport\\\",\\n    bins=10,\\n    binwidth=1,\\n)\\nsns.histplot(data=df, x=\\\"id_bucket_10\\\", color=\\\"red\\\", label=\\\"id\\\", bins=10, binwidth=1)\";\n",
       "                var nbb_formatted_code = \"sns.histplot(\\n    data=df,\\n    x=\\\"airport_bucket_10\\\",\\n    color=\\\"skyblue\\\",\\n    label=\\\"airport\\\",\\n    bins=10,\\n    binwidth=1,\\n)\\nsns.histplot(data=df, x=\\\"id_bucket_10\\\", color=\\\"red\\\", label=\\\"id\\\", bins=10, binwidth=1)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(\n",
    "    data=df,\n",
    "    x=\"airport_bucket_10\",\n",
    "    color=\"skyblue\",\n",
    "    label=\"airport\",\n",
    "    bins=10,\n",
    "    binwidth=1,\n",
    ")\n",
    "sns.histplot(data=df, x=\"id_bucket_10\", color=\"red\", label=\"id\", bins=10, binwidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9828fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"def hashed_train_test_split(df, index_col, test_size=0.2):\\n    \\\"\\\"\\\"\\n    Train-test split based on the hash of the unique identifier.\\n    \\\"\\\"\\\"\\n    test_index = df[index_col].apply(lambda x: crc32(np.int64(x)))\\n    test_index = test_index < test_size * 2 ** 32\\n\\n    return df.loc[~test_index], df.loc[test_index]\";\n",
       "                var nbb_formatted_code = \"def hashed_train_test_split(df, index_col, test_size=0.2):\\n    \\\"\\\"\\\"\\n    Train-test split based on the hash of the unique identifier.\\n    \\\"\\\"\\\"\\n    test_index = df[index_col].apply(lambda x: crc32(np.int64(x)))\\n    test_index = test_index < test_size * 2 ** 32\\n\\n    return df.loc[~test_index], df.loc[test_index]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def hashed_train_test_split(df, index_col, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Train-test split based on the hash of the unique identifier.\n",
    "    \"\"\"\n",
    "    test_index = df[index_col].apply(lambda x: crc32(np.int64(x)))\n",
    "    test_index = test_index < test_size * 2 ** 32\n",
    "\n",
    "    return df.loc[~test_index], df.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "460f46c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 800\n",
      "Test set: 200\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"# create an index column (should be immutable and unique)\\nX_1 = X_1.reset_index(drop=False)\\nX_2 = X_2.reset_index(drop=False)\\n\\n# apply the improved train-test split\\nX_1_train_hashed, X_1_test_hashed = hashed_train_test_split(X_1, \\\"index\\\")\\nX_2_train_hashed, X_2_test_hashed = hashed_train_test_split(X_2, \\\"index\\\")\\n\\n# see what is the overlap of indices\\nprint(\\n    f\\\"Train set: {len(set(X_1_train_hashed.index).intersection(set(X_2_train_hashed.index)))}\\\"\\n)\\nprint(\\n    f\\\"Test set: {len(set(X_1_test_hashed.index).intersection(set(X_2_test_hashed.index)))}\\\"\\n)\\n\\n# Train set: 800\\n# Test set: 200\";\n",
       "                var nbb_formatted_code = \"# create an index column (should be immutable and unique)\\nX_1 = X_1.reset_index(drop=False)\\nX_2 = X_2.reset_index(drop=False)\\n\\n# apply the improved train-test split\\nX_1_train_hashed, X_1_test_hashed = hashed_train_test_split(X_1, \\\"index\\\")\\nX_2_train_hashed, X_2_test_hashed = hashed_train_test_split(X_2, \\\"index\\\")\\n\\n# see what is the overlap of indices\\nprint(\\n    f\\\"Train set: {len(set(X_1_train_hashed.index).intersection(set(X_2_train_hashed.index)))}\\\"\\n)\\nprint(\\n    f\\\"Test set: {len(set(X_1_test_hashed.index).intersection(set(X_2_test_hashed.index)))}\\\"\\n)\\n\\n# Train set: 800\\n# Test set: 200\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create an index column (should be immutable and unique)\n",
    "X_1 = X_1.reset_index(drop=False)\n",
    "X_2 = X_2.reset_index(drop=False)\n",
    "\n",
    "# apply the improved train-test split\n",
    "X_1_train_hashed, X_1_test_hashed = hashed_train_test_split(X_1, \"index\")\n",
    "X_2_train_hashed, X_2_test_hashed = hashed_train_test_split(X_2, \"index\")\n",
    "\n",
    "# see what is the overlap of indices\n",
    "print(\n",
    "    f\"Train set: {len(set(X_1_train_hashed.index).intersection(set(X_2_train_hashed.index)))}\"\n",
    ")\n",
    "print(\n",
    "    f\"Test set: {len(set(X_1_test_hashed.index).intersection(set(X_2_test_hashed.index)))}\"\n",
    ")\n",
    "\n",
    "# Train set: 800\n",
    "# Test set: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f84f210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"from zlib import crc32\\n\\n\\ndef test_set_check(identifier, test_ratio):\\n    return crc32(np.int64(identifier)) & 0xFFFFFFFF < test_ratio * 2 ** 32\\n\\n\\ndef split_train_test_by_id(data, test_ratio, id_column):\\n    ids = data[id_column]\\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\\n    return data.loc[~in_test_set], data.loc[in_test_set]\";\n",
       "                var nbb_formatted_code = \"from zlib import crc32\\n\\n\\ndef test_set_check(identifier, test_ratio):\\n    return crc32(np.int64(identifier)) & 0xFFFFFFFF < test_ratio * 2 ** 32\\n\\n\\ndef split_train_test_by_id(data, test_ratio, id_column):\\n    ids = data[id_column]\\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\\n    return data.loc[~in_test_set], data.loc[in_test_set]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from zlib import crc32\n",
    "\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xFFFFFFFF < test_ratio * 2 ** 32\n",
    "\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cafe3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c787500c",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "\n",
    "- It is rare that rows are independent which could lead to data leakage, e.g. predicting flight delays -- need to make sure all information about a single day is in the training set (or testing set)\n",
    "- We want to ensure certain splits of the data always fall in training or test\n",
    "- Random seeds are not portable across languages or frameworks\n",
    "  - imagine you want to put a model into production and your engineer wants to recode it in Java or another language for performance reasons. You might struggle to reproduce the results\n",
    "  - we want lightweight, repeatrable splitting of the data which works regardless of language or random seeds and ensure correlated rows fall into the same split\n",
    "  - your colleague might forget to use a random seed or use their favourite (mine is 42 :) ), don't leave it to chance and add another place for divergence\n",
    "  \n",
    "- How can you design your split in order to make sure some values are only in the training or test set? Use hashing\n",
    "\n",
    "\"To have a stable train/test split even after updating the dataset, a common solution is to use each instance’s identifier to decide whether or not it should go in the test set (assuming instances have a unique and immutable identifier). For example, you could compute a hash of each instance’s identifier and put that instance in the test set if the hash is lower than or equal to 20% of the maximum hash value. This ensures that the test set will remain consistent across multiple runs, even if you refresh the dataset. The new test set will contain 20% of the new instances, but it will not contain any instance that was previously in the training set.\"\n",
    "\n",
    "\n",
    "\n",
    "### What is the problem with train_test_split?\n",
    "\n",
    "Randomly shuffles the data.\n",
    "\n",
    "\"Aha! But I always set the random seed so it is reproducible\" you might say. Fair point. This certainly improves the situation. However, not for all cases, particularly if you add new data to your dataset an retrain your model. \n",
    "\n",
    "\n",
    "### When might train_test_split not be appropriate?\n",
    "\n",
    "- If you add new data\n",
    "  - training the model on original data + new data\n",
    "  - if you are getting your source data from an evolving source and want to reproducibly sample from it\n",
    "- Of you can't guarantee the ording of datapoints (or filepaths) between training runs\n",
    "- If there is the possibility of data leakage\n",
    "  - ie. if you have correlated rows (e.g. airline lateness dataset)\n",
    "  - \"For example, if you are training a model to predict flight delays, the arrival delays of flights on the same day will be highly correlated with each other. This is called leakage, and it is an important problem to avoid when doing machine learning.\"\n",
    "- If you have large out of memory datasets\n",
    "- If your production code will be rewritten in another language -- you cannot guarantee the random seed in another language will produce the same results. This might make it hard to debug and validate the same results in the new code\n",
    "  - can also lead to difficult to catch bugs if in one part of the code you use sklearn's random seed and other places you use the inbuilt random module functionality.\n",
    "  \n",
    "Hashing is deterministic and will return the same result each time\n",
    "\n",
    "\n",
    "### What are the consequences of using random seed when retraining the model?\n",
    "- difficult to debug\n",
    "- data will be shuffled and in different splits. Makes it difficult to accurately compare training runs\n",
    "\n",
    "\n",
    "### When might hashing be unecessary?\n",
    "\n",
    "- standard timeseries data: just order your dataset and split\n",
    "- simple 'one-off' model training on 'small' datasets that easily fit in memory --> this is why it is the defacto method for most tutorials. In the real-world you need to consider whether this is always appropriate\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Should you go and always use hashing for train/test splits. Probably not. Using random seeds with `train_test_split()` method can be ok for one off training (although bear in mind it shuffles the data which might not work for timeseries data).\n",
    "\n",
    "However, like with many things in ML splitting the data is nuanced and can have undesired effects further down the line. Make sure you make a deliberate choice on the most appropriate method.\n",
    "\n",
    "\n",
    "I am proposing hashing as an alternative which might be more appropriate in certain situations. Should you use hashing for everything? No. But at least consider whether it is a better approach, particularly if you are retraining you model on old and new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3521741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bc2b386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jnes/tensorflow_datasets/imdb_reviews/plain_text/1.0.0/features.json\n",
      "/Users/jnes/tensorflow_datasets/imdb_reviews/plain_text/1.0.0/dataset_info.json\n",
      "/Users/jnes/tensorflow_datasets/imdb_reviews/plain_text/1.0.0/imdb_reviews-train.tfrecord-00000-of-00001\n",
      "/Users/jnes/tensorflow_datasets/imdb_reviews/plain_text/1.0.0/imdb_reviews-unsupervised.tfrecord-00000-of-00001\n",
      "/Users/jnes/tensorflow_datasets/imdb_reviews/plain_text/1.0.0/label.labels.txt\n",
      "/Users/jnes/tensorflow_datasets/imdb_reviews/plain_text/1.0.0/imdb_reviews-test.tfrecord-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "for file in glob.glob(\n",
    "    \"/Users/jnes/tensorflow_datasets/imdb_reviews/plain_text/1.0.0/*\"\n",
    "):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5775ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.json\n",
      "dataset_info.json\n",
      "imdb_reviews-train.tfrecord-00000-of-00001\n",
      "imdb_reviews-unsupervised.tfrecord-00000-of-00001\n",
      "label.labels.txt\n",
      "imdb_reviews-test.tfrecord-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for file in os.listdir(\n",
    "    \"/Users/jnes/tensorflow_datasets/imdb_reviews/plain_text/1.0.0/\"\n",
    "):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2647ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "268e3586",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "- use hashing\n",
    "- deterministic instead of probabilistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb39fdf",
   "metadata": {},
   "source": [
    "### Why farmhash?\n",
    "\n",
    "- Performance considerations\n",
    "- [Understanding hashing functions](https://github.com/google/farmhash/blob/master/Understanding_Hash_Functions)\n",
    "- [note around order of ABS and MOD](https://mentin.medium.com/be-careful-with-abs-function-8e91c78715d5)\n",
    "- Implemented consistently in C++ (and hence: Java or Python) and in BigQuery SQL.\n",
    "\n",
    "You could use another library such as `hashlib` using the md5 algorithm as suggested [in the Hands on with sklearn book](https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb). However, I find this implementation unnecessarily verbose.\n",
    "\n",
    "\n",
    "#### Order of operations\n",
    "\n",
    "Note that we do the modulo first, and then the absolute value: \n",
    "```sql\n",
    "CREATE TEMPORARY FUNCTION hashed ( \n",
    "    airport STRING , numbuckets INT64 \n",
    ") AS ( \n",
    "    ABS ( MOD ( FARM_FINGERPRINT ( airport ), numbuckets )) \n",
    "); \n",
    "```\n",
    "\n",
    "The order of ABS , MOD , and FARM_FINGERPRINT in the preceding snippet is important because the range of INT64 is not symmetric. Specifically, its range is between –9,223,372,036,854,775,808 and 9,223,372,036,854,775,807 (both inclusive). So, if we were to do: \n",
    "\n",
    "```sql\n",
    "ABS ( FARM_FINGERPRINT ( airport )) \n",
    "```\n",
    "\n",
    "we would run into a rare and likely unreproducible overflow error if the FARM_FINGERPRINT operation happened to return –9,223,372,036,854,775,808 since its absolute value can not be represented using an INT64! \n",
    "\n",
    "---\n",
    "#### Cryptographic hash\n",
    "\n",
    "What makes the Hashed Feature lossy is the modulo part of the implementation. What if we were to avoid the modulo altogether? After all, the farm fingerprint has a fixed length (an INT64 is 64 bits), and so it can be represented using 64 feature values, each of which is 0 or 1. This is called binary encoding . However, binary encoding does not solve the problem of out-of-vocabulary inputs or cold start (only the problem of high cardinality). In fact, the bitwise coding is a red herring. If we don’t do a modulo, we can get a unique representation by simply encoding the three characters that form the IATA code (thus using a feature of length 3*26=78). The problem with this representation is immediately obvious: airports whose names start with the letter O have nothing in common when it comes to their flight delay characteristics the encoding has created a spurious correlation between airports that start with the same letter. The same insight holds in binary space as well. Because of this, we do not recommend binary encoding of farm fingerprint values. \n",
    "\n",
    "Binary encoding of an MD5 hash will not suffer from this spurious correlation problem because the output of an MD5 hash is uniformly distributed, and so the resulting bits will be uniformly distributed. However, unlike the Farm Fingerprint algorithm, the MD5 hash is not deterministic and not unique it is a one-way hash and will have many unexpected collisions. \n",
    "\n",
    "In the Hashed Feature design pattern, we have to use a fingerprint hashing algorithm and not a cryptographic hashing algorithm. This is because the goal of a fingerprint function is to produce a deterministic and unique value. If you think about it, this is a key requirement of preprocessing functions in machine learning, since we need to apply the same function during model serving and get the same hashed value. A fingerprint function does not produce a uniformly distributed output. Cryptographic algorithms such as MD5 or SHA1 do produce uniformly distributed output, but they are not deterministic and are purposefully made to be computationally expensive. Therefore, a cryptographic hash is not usable in a feature engineering context where the hashed value computed for a given input during prediction has to be the same as the hash computed during training, and where the hash function should not slow down the machine learning model. \n",
    "\n",
    "\"The reason that MD5 is not deterministic is that a “salt” is typically added to the string to be hashed. The salt is a random string added to each password to ensure that even if two users happen to use the same password, the hashed value in the database will be different. This is needed to thwart attacks based on “rainbow tables,” which are attacks that rely on dictionaries of commonly chosen passwords and that compare the hash of the known password against hashes in the database. As computational power has increased, it is possible to carry out a brute-force attack on every possible salt as well, and so modern cryptographic implementations do their hashing in a loop to increase the computational expense. Even if we were to turn off the salt and reduce the number of iterations to one, the MD5 hash is only one way. It won’t be unique.\"\n",
    "\n",
    "\n",
    "Lakshmanan, Valliappa,Robinson, Sara,Munn, Michael. Machine Learning Design Patterns (p. 37). O'Reilly Media. Kindle Edition. \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Which column should you hash?\n",
    "\n",
    "Use a unique identifier. Concatenate columns if this is not possible.\n",
    "\n",
    "Should not be a field which is used for the model.\n",
    "\n",
    "If splitting on date, we can still use features extracted from it (e.g. day of week, month etc.), but not the date itself (which makes sense anyway as it would not make sense to use a historical date as a feature for dates in the future).\n",
    "\n",
    "\"We can automate checking whether the label distributions are similar across the three datasets by using the Kolomogorov–Smirnov test: just plot the cumulative distribution functions of the label in the three datasets and find the maximum distance between each pair. The smaller the maximum distance, the better the split.\"\n",
    "\n",
    "### Considerations\n",
    "- what are the gotchas?\n",
    "- with small datasets you won't have exactly the % split you desire. -- I could do a chart showing how when splitting on ID it eventually trends to the correct split if data is large enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3fb85f",
   "metadata": {},
   "source": [
    "### Demo\n",
    "\n",
    "- use fake properties data (to demonstrate hashing of strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b06d4ed",
   "metadata": {},
   "source": [
    "### Other use cases for hashing\n",
    "\n",
    "- feature engineering categorical values\n",
    "- using reproducible subsets of large data for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bc8e3d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 1 must be str, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m TEST_LENGTH\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e4\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhashlib\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mfarmhash\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhash64\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m123\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 1 must be str, not int"
     ]
    }
   ],
   "source": [
    "import farmhash\n",
    "\n",
    "TEST_LENGTH = 1e4\n",
    "\n",
    "import hashlib\n",
    "\n",
    "farmhash.hash64(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd84498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 5.72 µs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(TEST_LENGTH)):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mfarmhash\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhash64\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time\n",
    "for _ in range(int(TEST_LENGTH)):\n",
    "    farmhash.hash64(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b13ece38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unicode-objects must be encoded before hashing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(TEST_LENGTH)):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mhashlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmd5\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unicode-objects must be encoded before hashing"
     ]
    }
   ],
   "source": [
    "%time\n",
    "for _ in range(int(TEST_LENGTH)):\n",
    "    hashlib.md5(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f276f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
